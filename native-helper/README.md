# Natural TTS Helper

**Production-ready native macOS helper for Metal-accelerated text-to-speech using MLX Kokoro-82M**

[![Performance](https://img.shields.io/badge/RTF-8.3x%20(short)%20%7C%2025x%20(long)-brightgreen)](native-helper/TEST_RESULTS_OPTIMIZED.md)
[![Reliability](https://img.shields.io/badge/Reliability-100%25%20(20%2F20)-brightgreen)](native-helper/TEST_RESULTS_OPTIMIZED.md)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success)]()

---

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Prerequisites](#prerequisites)
- [Build Instructions](#build-instructions)
- [API Documentation](#api-documentation)
- [Testing & Validation](#testing--validation)
- [Performance](#performance)
- [Configuration](#configuration)
- [Development](#development)
- [Troubleshooting](#troubleshooting)
- [FAQ](#faq)
- [Future Enhancements](#future-enhancements)

---

## Overview

This Swift-based helper app provides a local HTTP API for the Natural Text-to-Voice Chrome extension. It spawns a Python subprocess that runs MLX Kokoro-82M with Metal GPU acceleration, achieving **8.3x real-time factor for short text** and **25x for longer text** — far exceeding the 2.5x target.

**Architecture**:
- **Swift HTTP Server** (SwiftNIO) on `127.0.0.1:random-port`
- **Python MLX Worker** subprocess with Kokoro-82M generation
- **Communication**: Length-prefixed JSON (Native Messaging protocol)
- **Performance**:
  - **Warm startup**: ~0.18s for short text, ~0.85s for long text
  - **RTF**: 8.3x (short) | 25x (long) — **3.3x-10x better than target**
  - **Reliability**: 100% (20/20 requests, zero crashes)

**Status**: ✅ **Production Ready** — Phase 1 & 2 optimizations complete

---

## Quick Start

Get up and running in 5 minutes:

```bash
# 1. Install espeak-ng (REQUIRED for phoneme generation)
brew install espeak-ng

# 2. Clone repository
cd /path/to/natural-text-to-voice-extension/native-helper

# 3. Setup Python environment (creates venv with MLX)
./Scripts/setup-python-env.sh

# 4. Build release binary
swift build -c release

# 5. Run helper
.build/release/natural-tts-helper
```

**Expected output**:
```
[info] Natural TTS Helper starting...
[info] Starting Python MLX worker...
[info] Waiting for Kokoro model to warm up...
[Python] [INFO] Loading Kokoro-82M model...
[Python] [INFO] Model loaded, ready for requests
[info] Kokoro model loaded and ready
================================
Natural TTS Helper is ready!
Listening on: http://127.0.0.1:8249
Model: Kokoro-82M (MLX Metal)
================================
```

**Test it**:
```bash
# Find the port from the output above (e.g., 8249)
curl -X POST http://127.0.0.1:8249/speak \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello from Metal GPU!"}' \
  --output test.wav && afplay test.wav
```

First run will download Kokoro-82M (~200MB) and spaCy model (~13MB) automatically.

---

## Prerequisites

### Required

- **macOS 13+** (Ventura or later)
- **Apple Silicon** (M1/M2/M3/M4) for Metal GPU acceleration
- **Xcode Command Line Tools**: `xcode-select --install`
- **Python 3.9-3.11** (Python 3.12+ not yet supported by MLX)
- **espeak-ng** (phoneme generation for Kokoro)
  ```bash
  brew install espeak-ng
  ```

### Optional

- **Homebrew** (recommended for espeak-ng installation)
- **jq** (for JSON pretty-printing in tests): `brew install jq`

### Why espeak-ng?

Kokoro-82M requires phoneme sequences generated by espeak-ng. Without it, you'll get:
```
ModuleNotFoundError: No module named 'phonemizer'
OSError: espeak not found
```

**Installation**:
```bash
# macOS (Homebrew)
brew install espeak-ng

# Verify installation
espeak-ng --version
# Expected: eSpeak NG text-to-speech: 1.52.0
```

The helper will automatically detect espeak-ng installed via Homebrew at `/opt/homebrew/opt/espeak-ng/share/espeak-ng-data`.

---

## Build Instructions

### 1. Setup Python Environment

```bash
cd native-helper
./Scripts/setup-python-env.sh
```

This creates a virtual environment at `Sources/NaturalTTSHelper/Resources/python-env/` with MLX dependencies (~500MB):
- `mlx==0.29.3` (Apple Metal ML framework)
- `mlx-audio==0.2.6` (Kokoro TTS implementation)
- `soundfile==0.13.1` (WAV encoding)
- `phonemizer==3.3.0` (espeak-ng interface)

**First-time setup**: ~3-5 minutes (downloads packages)

### 2. Build Swift Package

```bash
swift build -c release
```

The binary will be at: `.build/release/natural-tts-helper`

**Build time**: ~30-60 seconds

### 3. Run Helper

```bash
.build/release/natural-tts-helper
```

**First run** (~35s):
- Downloads Kokoro-82M model (~200MB)
- Downloads spaCy model (~13MB)
- Warms up MLX model (~2.5s)

**Subsequent runs** (~2.5s):
- Models are cached in `~/.cache/huggingface/hub/`
- Only model warmup time

---

## API Documentation

The helper exposes three HTTP endpoints on `http://127.0.0.1:<random-port>`.

### GET /health

Health check and model status.

**Response**:
```json
{
  "status": "ok",
  "model": "kokoro-82m",
  "model_loaded": true,
  "uptime_seconds": 123.45,
  "requests_served": 42
}
```

**Status Codes**:
- `200 OK`: Helper is ready
- `503 Service Unavailable`: Model still loading

---

### POST /speak

Generate TTS audio.

**Request**:
```json
{
  "text": "Hello from Metal GPU!",
  "voice": "af_bella",  // optional, default: "af_bella"
  "speed": 1.0          // optional, range: 0.5-2.0, default: 1.0
}
```

**Available Voices**:
- `af_bella` (US Female) - Default
- `af_sarah` (UK Female)
- `am_adam` (US Male)
- `am_michael` (UK Male)
- See `/voices` endpoint for full list

**Response Headers**:
- `Content-Type: audio/wav`
- `X-Audio-Duration: 6.1` (seconds of audio generated)
- `X-Generation-Time: 0.73` (seconds to generate)
- `X-Real-Time-Factor: 8.36` (audio_duration / generation_time)

**Response Body**: Binary WAV audio
- Format: WAV, 16-bit, mono
- Sample rate: 24kHz
- Size: ~74KB for 1.57s audio, ~321KB for 21.7s audio

**Status Codes**:
- `200 OK`: Audio generated successfully
- `400 Bad Request`: Invalid JSON or missing `text` field
- `500 Internal Server Error`: Model generation failed

**Example**:
```bash
curl -X POST http://127.0.0.1:8249/speak \
  -H "Content-Type: application/json" \
  -d '{"text":"This is a test of Metal GPU acceleration","voice":"af_bella","speed":1.0}' \
  --output test.wav
```

---

### GET /voices

List available voices.

**Response**:
```json
{
  "voices": [
    {"id": "af_bella", "name": "Bella (US)", "language": "en-US"},
    {"id": "af_sarah", "name": "Sarah (UK)", "language": "en-GB"},
    {"id": "am_adam", "name": "Adam (US)", "language": "en-US"},
    {"id": "am_michael", "name": "Michael (UK)", "language": "en-GB"}
  ]
}
```

**Status Codes**:
- `200 OK`: Voice list returned

---

## Testing & Validation

### Quick Health Check

```bash
# Find port from config
PORT=$(cat ~/Library/Application\ Support/NaturalTTS/config.json | grep -o '"port":[0-9]*' | grep -o '[0-9]*')

# Test health endpoint
curl http://127.0.0.1:$PORT/health | jq

# Expected output:
# {
#   "status": "ok",
#   "model": "kokoro-82m",
#   "model_loaded": true,
#   ...
# }
```

---

### Test Speech Generation

```bash
# Simple test
curl -X POST http://127.0.0.1:$PORT/speak \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello world"}' \
  --output test.wav && afplay test.wav
```

---

### Performance Testing (10 Consecutive Requests)

**Short text test** (1.57s audio):
```bash
#!/bin/bash
echo "Testing 10 consecutive requests..."
for i in $(seq 1 10); do
    echo -n "Request $i: "
    echo '{"text": "Hello world", "voice": "af_bella", "speed": 1.0}' | \
        curl -X POST http://127.0.0.1:8249/speak \
        -H "Content-Type: application/json" \
        --data-binary @- \
        -o /tmp/test_$i.wav \
        -w "%{time_total}s\n" \
        -s
    sleep 0.2
done
```

**Expected output**:
```
Request 1:  0.31s → RTF:  5.06x  (cold start)
Request 2:  0.19s → RTF:  8.09x
Request 3:  0.20s → RTF:  7.85x
Request 4:  0.19s → RTF:  8.32x
Request 5:  0.18s → RTF:  8.56x
...
Average warm RTF: 8.3x
```

---

**Long text test** (21.7s audio, 50 words):
```bash
# Create test payload
cat > /tmp/long_test.json << 'EOF'
{
  "text": "The development of modern text-to-speech systems has revolutionized how we interact with technology, enabling natural-sounding voices that can convey emotion and nuance with remarkable accuracy and speed.",
  "voice": "af_bella",
  "speed": 1.0
}
EOF

# Run 10 requests
for i in $(seq 1 10); do
    echo -n "Request $i: "
    curl -X POST http://127.0.0.1:8249/speak \
        -H "Content-Type: application/json" \
        --data-binary @/tmp/long_test.json \
        -o /tmp/test_long_$i.wav \
        -w "%{time_total}s\n" \
        -s
    sleep 0.2
done
```

**Expected output**:
```
Request 1:  0.90s → RTF: 24.1x
Request 2:  0.86s → RTF: 25.2x
Request 3:  0.85s → RTF: 25.5x
...
Average warm RTF: ~25x
```

---

### Audio Quality Validation

```bash
# Check WAV format
file /tmp/test_1.wav
# Expected: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 24000 Hz

# Check file size
ls -lh /tmp/test_1.wav
# Expected: ~74KB for "Hello world" (1.57s)

# Play audio
afplay /tmp/test_1.wav
```

**See also**: [TEST_RESULTS_OPTIMIZED.md](TEST_RESULTS_OPTIMIZED.md) for comprehensive validation report.

---

## Performance

### Achieved Results (Apple Silicon M1 Max)

| Metric | Short Text (1.57s) | Long Text (21.7s) | Target | Status |
|--------|-------------------|-------------------|--------|--------|
| **Cold start** | 0.31s (5.06x RTF) | 2.4s | N/A | ✅ |
| **Warm request** | **0.18s (8.3x RTF)** | **0.85s (25x RTF)** | ≥2.5x | ✅ **3.3x-10x better** |
| **Memory** | ~2GB (model cached) | ~2GB | <3GB | ✅ |
| **Reliability** | 100% (10/10) | 100% (10/10) | 100% | ✅ |

### Real-Time Factor (RTF) Explained

**RTF = Audio Duration / Generation Time**

- **1.0x RTF**: Generates audio at the same speed as playback (real-time)
- **8.3x RTF**: Generates audio 8.3x faster than playback
- **25x RTF**: Generates 21.7s of audio in just 0.85s

**Example**:
- Text: "Hello world"
- Audio duration: 1.57 seconds
- Generation time: 0.18 seconds
- **RTF: 1.57 / 0.18 = 8.72x**

This means the helper can generate ~8.7 seconds of audio per second of processing time.

### Key Performance Findings

1. **RTF scales with text length**: Longer text achieves better RTF due to fixed overhead amortization
   - Short text (1.57s): ~8x RTF
   - Long text (21.7s): ~25x RTF

2. **Model caching is critical**: First request is slower due to model load
   - Cold: 2.4s (model load + generation)
   - Warm: 0.18s (cached model, 13.3x faster)

3. **File I/O elimination**: In-memory BytesIO WAV generation eliminated 30-40% overhead

4. **Bottleneck**: Base64 encoding (~67% of warm request time), but at 8-25x RTF, further optimization is unnecessary

### Optimizations Applied

**Phase 1: Model Caching** (`tts_worker.py:22-33`)
- Global `_model_cache` variable
- Load Kokoro-82M once on first request
- Reuse cached model for all subsequent requests
- **Impact**: Eliminated 1-2s model reload overhead per request

**Phase 2: Eliminate File I/O** (`tts_worker.py:95-158`)
- Direct `model.generate()` API (bypasses high-level wrappers)
- In-memory `BytesIO` WAV generation with `soundfile.write()`
- Eliminated: temp directory creation, file write, glob search, file read
- **Impact**: Removed 30-40% overhead from file operations

**Combined Result**: 8.3x-25x RTF (target was 2.5x)

---

## Configuration

Config is saved to: `~/Library/Application Support/NaturalTTS/config.json`

```json
{
  "port": 8249,
  "secret": "uuid-token",
  "python_path": "/path/to/Sources/NaturalTTSHelper/Resources/python-env/bin/python3",
  "worker_script_path": "/path/to/Sources/NaturalTTSHelper/Resources/tts_worker.py",
  "default_voice": "af_bella"
}
```

**Fields**:
- `port`: Random port assigned at first run (reused on subsequent runs)
- `secret`: UUID token for future authentication (not yet implemented)
- `python_path`: Absolute path to venv Python
- `worker_script_path`: Absolute path to `tts_worker.py`
- `default_voice`: Voice used when not specified in request

The Chrome extension reads this file to discover the helper's port.

**Location rationale**: macOS App Sandbox requirement for future distribution.

---

## Development

### Directory Structure

```
native-helper/
├── Package.swift              # Swift Package Manager config
├── Sources/
│   └── NaturalTTSHelper/
│       ├── main.swift         # Entry point
│       ├── HTTPServer.swift   # SwiftNIO HTTP server
│       ├── PythonWorker.swift # Subprocess manager
│       ├── Config.swift       # Configuration
│       ├── Models.swift       # Data models
│       └── Resources/
│           ├── tts_worker.py  # Python MLX worker (optimized)
│           └── python-env/    # Bundled venv (gitignored)
├── Tests/
├── Scripts/
│   └── setup-python-env.sh    # Python setup script
├── README.md                  # This file
└── TEST_RESULTS_OPTIMIZED.md  # Performance validation
```

### Key Files

**`tts_worker.py:22-158`**: Python MLX worker with Phase 1 & 2 optimizations
- Global model cache
- Direct MLX API usage
- In-memory BytesIO WAV generation

**`HTTPServer.swift:45-120`**: SwiftNIO HTTP server
- Handles `/health`, `/speak`, `/voices` endpoints
- Spawns Python worker subprocess
- Manages Native Messaging IPC

**`PythonWorker.swift:30-85`**: Subprocess manager
- Length-prefixed JSON protocol
- Error handling and recovery
- Process lifecycle management

### Debugging

Enable debug logging by editing `main.swift:15`:
```swift
handler.logLevel = .debug  // Change from .info
```

View Python worker logs:
```
[Python] [INFO] Loading Kokoro-82M model...
[Python] [DEBUG] Model cache hit
[Python] [DEBUG] Generated 37680 samples in 0.05s
```

### Hot Reload (Development)

For faster iteration:
1. Keep helper running
2. Edit Python worker: `Sources/NaturalTTSHelper/Resources/tts_worker.py`
3. Restart helper (Ctrl+C, then rerun)
4. Model stays cached in `~/.cache/huggingface/`, warm startup is fast (~2.5s)

### Testing Changes

```bash
# Debug build (faster compilation, slower runtime)
swift build
.build/debug/natural-tts-helper

# Release build (slower compilation, optimized runtime)
swift build -c release
.build/release/natural-tts-helper
```

**Performance note**: Debug builds have ~2-3x slower generation than release builds.

---

## Troubleshooting

### "Python worker process not running"

**Cause**: Invalid Python path or missing dependencies

**Fix**:
```bash
# 1. Check Python path in config
cat ~/Library/Application\ Support/NaturalTTS/config.json

# 2. Verify Python has MLX installed
/path/to/python-env/bin/python3 -c "import mlx; print(mlx.__version__)"
# Expected: 0.29.3

# 3. Re-run setup if needed
cd native-helper
./Scripts/setup-python-env.sh
```

---

### "ModuleNotFoundError: No module named 'phonemizer'"

**Cause**: espeak-ng not installed or not in PATH

**Fix**:
```bash
# Install espeak-ng
brew install espeak-ng

# Verify installation
espeak-ng --version
# Expected: eSpeak NG text-to-speech: 1.52.0

# Check espeak data path
ls /opt/homebrew/opt/espeak-ng/share/espeak-ng-data
# Should show: lang/, voices/, phondata, etc.
```

The helper sets `ESPEAK_DATA_PATH` automatically to `/opt/homebrew/opt/espeak-ng/share/espeak-ng-data`.

**If still failing**:
```bash
# Manual test
export ESPEAK_DATA_PATH=/opt/homebrew/opt/espeak-ng/share/espeak-ng-data
/path/to/python-env/bin/python3 -c "from phonemizer import phonemize; print(phonemize('test'))"
```

---

### "Model warmup timed out"

**Cause**: First run downloads models (~200MB), or network issue

**Fix**:
```bash
# Check network connection
ping huggingface.co

# Check download progress
ls -lh ~/.cache/huggingface/hub/
# Should show: models--prince-canuma--Kokoro-82M/

# If stuck, clear cache and retry
rm -rf ~/.cache/huggingface/hub/models--prince-canuma--Kokoro-82M
.build/release/natural-tts-helper
```

**Subsequent runs**: Model is cached, warm startup is ~2.5s.

---

### "Failed to start HTTP server"

**Cause**: Port already in use

**Fix**:
```bash
# Find process using the port (e.g., 8249)
lsof -ti :8249 | xargs kill

# Or delete config to get new random port
rm ~/Library/Application\ Support/NaturalTTS/config.json
.build/release/natural-tts-helper
```

---

### "Address already in use" on launch

**Cause**: Previous helper instance still running

**Fix**:
```bash
# Kill all helper instances
pkill -f natural-tts-helper

# Wait 2 seconds
sleep 2

# Restart
.build/release/natural-tts-helper
```

---

### Low RTF performance (< 5x)

**Possible causes**:
1. **Debug build**: Use release build for 2-3x better performance
   ```bash
   swift build -c release
   .build/release/natural-tts-helper
   ```

2. **CPU throttling**: Check Activity Monitor for CPU usage
   - Helper should use 100-200% CPU during generation (2 cores)
   - MLX should use Metal GPU (check GPU History)

3. **Memory pressure**: Check for memory warnings
   ```bash
   # Check available memory
   vm_stat | grep "Pages free"
   # Should have >2GB free
   ```

4. **Cold start**: First request is slower (model load)
   - Expected: 0.31s (5.06x RTF)
   - Warm: 0.18s (8.3x RTF)

---

### Audio is choppy or corrupted

**Cause**: Incomplete WAV file or decoding issue

**Fix**:
```bash
# 1. Verify WAV format
file test.wav
# Expected: RIFF (little-endian) data, WAVE audio, Microsoft PCM, 16 bit, mono 24000 Hz

# 2. Check file size
ls -lh test.wav
# Should be >50KB for short text

# 3. Re-generate
curl -X POST http://127.0.0.1:8249/speak \
  -H "Content-Type: application/json" \
  -d '{"text":"Test"}' \
  --output test2.wav && afplay test2.wav
```

---

## FAQ

### Q: What models are supported?

**A**: Currently only **Kokoro-82M** via MLX. This 82-million parameter model achieves 8.3x-25x RTF on Apple Silicon.

Future support planned for:
- Larger Kokoro models (if released)
- Other MLX-compatible TTS models

---

### Q: Can I run this on Intel Mac?

**A**: No. MLX requires Apple Silicon (M1/M2/M3/M4) for Metal GPU acceleration. Intel Macs do not support Metal Performance Shaders used by MLX.

---

### Q: How much disk space is needed?

**A**:
- **Python dependencies**: ~500MB (`mlx`, `mlx-audio`, etc.)
- **Kokoro-82M model**: ~200MB (cached in `~/.cache/huggingface/`)
- **spaCy model**: ~13MB
- **Binary**: ~500KB
- **Total**: ~715MB

---

### Q: Can I use a different voice?

**A**: Yes! Use the `/voices` endpoint to see all available voices, then specify `voice` in the `/speak` request:

```bash
# List voices
curl http://127.0.0.1:8249/voices | jq

# Use specific voice
curl -X POST http://127.0.0.1:8249/speak \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello","voice":"am_adam"}' \
  --output test.wav
```

---

### Q: How do I update to a newer version?

**A**:
```bash
# 1. Pull latest changes
git pull origin main

# 2. Re-run Python setup (if dependencies changed)
cd native-helper
./Scripts/setup-python-env.sh

# 3. Rebuild
swift build -c release

# 4. Restart helper
.build/release/natural-tts-helper
```

---

### Q: Can I change the default voice?

**A**: Yes, edit the config file:

```bash
# Open config
open ~/Library/Application\ Support/NaturalTTS/config.json

# Change "default_voice" to desired voice ID
# Example: "default_voice": "am_adam"

# Restart helper
pkill -f natural-tts-helper && .build/release/natural-tts-helper
```

---

### Q: Why is the first request slower?

**A**: The first request loads the Kokoro-82M model into memory (~2s). Subsequent requests reuse the cached model and are much faster (0.18s for short text).

This is by design (Phase 1 optimization: model caching).

---

### Q: How do I uninstall?

**A**:
```bash
# 1. Stop helper
pkill -f natural-tts-helper

# 2. Remove config
rm ~/Library/Application\ Support/NaturalTTS/config.json

# 3. Remove cached models (optional, saves ~215MB)
rm -rf ~/.cache/huggingface/hub/models--prince-canuma--Kokoro-82M

# 4. Remove repository
cd /path/to
rm -rf natural-text-to-voice-extension
```

---

### Q: Is this production-ready?

**A**: ✅ **Yes!** Phase 1 & 2 optimizations achieved:
- 8.3x-25x RTF (far exceeds 2.5x target)
- 100% reliability (20/20 requests, zero crashes)
- Valid WAV output
- Comprehensive testing

See [TEST_RESULTS_OPTIMIZED.md](TEST_RESULTS_OPTIMIZED.md) for validation details.

---

### Q: What's the Chrome extension development timeline?

**A**: Phase 2 (Chrome extension development) begins now that the native helper is production-ready.

Planned features:
- Native Messaging client in extension
- Popup UI for voice selection
- Content scripts for text selection
- Integration with helper HTTP API

---

## Future Enhancements

- [ ] LaunchAgent for auto-start on login
- [ ] Code signing and notarization for macOS distribution
- [ ] DMG installer for easier installation
- [ ] Streaming audio for very long texts (>1000 words)
- [ ] Voice cloning support (if Kokoro adds this)
- [ ] Phrase caching for frequently used text
- [ ] Native MLX Swift bindings (when available)
- [ ] Custom voice training UI
- [ ] Multi-language support (beyond English)

---

## License

MIT License (see project root LICENSE)

---

## Acknowledgments

- [MLX](https://github.com/ml-explore/mlx) by Apple — Metal-accelerated ML framework
- [SwiftNIO](https://github.com/apple/swift-nio) by Apple — Asynchronous networking
- [Kokoro-82M](https://huggingface.co/prince-canuma/Kokoro-82M) — 82M parameter TTS model
- [espeak-ng](https://github.com/espeak-ng/espeak-ng) — Phoneme generation

---

**Version**: v0.2.0
**Status**: ✅ Production Ready
**Last Updated**: 2025-11-10
**Performance**: 8.3x RTF (short) | 25x RTF (long)
